{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from functools import reduce\n",
    "import nibabel as nib\n",
    "import pydicom as pdc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "autoencode_path = os.path.join('data', 'autoencode')\n",
    "classifier_path = os.path.join('data', 'classifier')\n",
    "\n",
    "if not os.path.exists(autoencode_path):\n",
    "    os.mkdir(autoencode_path)\n",
    "    os.mkdir(os.path.join(autoencode_path, 'train'))\n",
    "    os.mkdir(os.path.join(autoencode_path, 'valid'))\n",
    "if not os.path.exists(classifier_path):\n",
    "    os.mkdir(classifier_path)\n",
    "    os.mkdir(os.path.join(classifier_path, 'train'))\n",
    "    os.mkdir(os.path.join(classifier_path, 'valid'))\n",
    "    os.mkdir(os.path.join(classifier_path, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Brain MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_path = os.path.join('source', 'Normal')\n",
    "meta = pd.read_csv(os.path.join('source', 'flipped_clinical_NormalPedBrainAge_StanfordCohort.csv'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2 ax']\n",
    "meta = meta[meta['SeriesDescription']=='AX T2 FRFSE']\n",
    "meta = meta[meta['is_Duplicate']=='NO']\n",
    "normal_set = set(map(lambda x: os.path.join(healthy_path, '{}.npz').format(x),\n",
    "                     meta['Patient_ID'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'ST_DIPG_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 5 for y in x]) and len(x)<51, groups))\n",
    "dipg_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'ST_PF-EP_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 6 for y in x]) and len(x)<55, groups))\n",
    "ep_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'ST_PF-MB_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 6 for y in x]) and len(x) < 40, groups))\n",
    "mb_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PILO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'ST_PF-PILO_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 6 for y in x]) and len(x) < 68, groups))\n",
    "pilo_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seattle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'SE_DIPG_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 5 for y in x]) and len(x)<51, groups))\n",
    "se_dipg_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'SE_PF-EP_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 6 for y in x]) and len(x) < 68, groups))\n",
    "se_ep_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_excel(os.path.join(os.path.join('source', 'katie_annotated_metadata'),\n",
    "                                  'SE_PF-MB_private_all_metadata_with_roi_annotated.xlsx'))\n",
    "meta = meta[meta['Series']=='T2']\n",
    "meta = meta[meta['Plane']=='Axial']\n",
    "meta = meta[meta['ModelFilter']=='T2_Axial']\n",
    "meta['Filenames'] = meta['PID']+'-'+meta['SID'].apply(lambda x: '{:02d}'.format(x))+'-'+meta['FileName_df']\n",
    "meta['Group'] = meta['PID']+'_'+meta['FileName_df'].apply(lambda x: x.split('-')[1])\n",
    "groups = meta[['Group', 'Filenames']].groupby('Group')['Filenames'].apply(sorted).values.tolist()\n",
    "groups = list(filter(lambda x: all([len(y.split('-')) == 6 for y in x]) and len(x) < 68, groups))\n",
    "se_mb_set = set(reduce(lambda x, y: x + y, groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder Data Preprocessing\n",
    "\n",
    "***Using the BRATS public dataset and the normal healthy brain dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_path = os.path.join('source', 'Task01_BrainTumour')\n",
    "brats_train = os.path.join(brats_path, 'imagesTr')\n",
    "brats_valid = os.path.join(brats_path, 'imagesTs')\n",
    "\n",
    "healthy_path = os.path.join('source', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencode_files = []\n",
    "autoencode_types = []\n",
    "\n",
    "for _, _, files in os.walk(brats_train):\n",
    "    files = sorted(map(lambda x: os.path.join(brats_train, x),\n",
    "                       filter(lambda x: not x.startswith('.'), files)))\n",
    "    autoencode_files.extend(files)\n",
    "    autoencode_types.extend([1 for _ in range(len(files))])\n",
    "\n",
    "for _, _, files in os.walk(brats_valid):\n",
    "    files = sorted(map(lambda x: os.path.join(brats_valid, x),\n",
    "                       filter(lambda x: not x.startswith('.'), files)))\n",
    "    autoencode_files.extend(files)\n",
    "    autoencode_types.extend([1 for _ in range(len(files))])\n",
    "\n",
    "for _, _, files in os.walk(healthy_path):\n",
    "    files = set(map(lambda x: os.path.join(healthy_path, x),\n",
    "                       filter(lambda x: not x.startswith('.'), files)))\n",
    "    files = sorted(files.intersection(normal_set))\n",
    "    autoencode_files.extend(files)\n",
    "    autoencode_types.extend([2 for _ in range(len(files))])\n",
    "\n",
    "train_path, valid_path, train_type, valid_type = train_test_split(autoencode_files,\n",
    "                                                                  autoencode_types,\n",
    "                                                                  test_size=0.2,\n",
    "                                                                  random_state=seed,\n",
    "                                                                  stratify=autoencode_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencode_meta = {'min': float('inf'), 'max': float('-inf')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = os.path.join(autoencode_path, 'train')\n",
    "\n",
    "fp = open('ae_train.csv', 'w')\n",
    "fp.write('filepath, plane\\n')\n",
    "\n",
    "cnt = 0\n",
    "bar = ProgressBar(maxval=len(train_path), widgets=[Bar('=', '[', ']'), ' ', Percentage()]).start()\n",
    "\n",
    "for path, method in list(zip(train_path, train_type)):\n",
    "    img = None\n",
    "    if method==1:\n",
    "        img = nib.load(path).get_fdata()[:,:,25:125,3]\n",
    "        img = resize(img, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "    else:\n",
    "        img = np.load(path)['T2 ax']\n",
    "        dim1 = max(img.shape[:2]) - img.shape[0]\n",
    "        dim2 = max(img.shape[:2]) - img.shape[1]\n",
    "        if dim1 != 0 or dim2 != 0:\n",
    "            pad = np.pad(img, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                               (math.ceil(dim2/2.0),math.floor(dim2/2.0)), (0,0)),\n",
    "                         mode='constant', constant_values=0)\n",
    "            img = resize(pad, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2,0,1))\n",
    "        img = np.rot90(img, axes=(2,1))\n",
    "    \n",
    "    img = (img - img.mean())/img.std()\n",
    "    \n",
    "    min_img = img.min()\n",
    "    max_img = img.max()\n",
    "    if min_img < autoencode_meta['min']:\n",
    "        autoencode_meta['min'] = min_img\n",
    "    if max_img > autoencode_meta['max']:\n",
    "        autoencode_meta['max'] = max_img\n",
    "    \n",
    "    np.savez_compressed(os.path.join(data_train_path, '{:04d}'.format(cnt)), data=img)\n",
    "    for k in range(img.shape[0]):\n",
    "        fp.write('{}, {}\\n'.format(os.path.join(data_train_path, '{:04d}.npz'.format(cnt)), k))\n",
    "    cnt += 1\n",
    "    bar.update(cnt)\n",
    "\n",
    "bar.finish()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid_path = os.path.join(autoencode_path, 'valid')\n",
    "\n",
    "fp = open('ae_valid.csv', 'w')\n",
    "fp.write('filepath, plane\\n')\n",
    "\n",
    "cnt = 0\n",
    "bar = ProgressBar(maxval=len(valid_path), widgets=[Bar('=', '[', ']'), ' ', Percentage()]).start()\n",
    "\n",
    "for path, method in list(zip(valid_path, valid_type)):\n",
    "    img = None\n",
    "    if method==1:\n",
    "        img = nib.load(path).get_fdata()[:,:,25:125,3]\n",
    "        img = resize(img, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "    else:\n",
    "        img = np.load(path)['T2 ax']\n",
    "        dim1 = max(img.shape[:2]) - img.shape[0]\n",
    "        dim2 = max(img.shape[:2]) - img.shape[1]\n",
    "        if dim1 != 0 or dim2 != 0:\n",
    "            pad = np.pad(img, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                               (math.ceil(dim2/2.0),math.floor(dim2/2.0)), (0,0)),\n",
    "                         mode='constant', constant_values=0)\n",
    "            img = resize(pad, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2,0,1))\n",
    "        img = np.rot90(img, axes=(2,1))\n",
    "    \n",
    "    img = (img - img.mean())/img.std()\n",
    "    \n",
    "    np.savez_compressed(os.path.join(data_valid_path, '{:04d}'.format(cnt)), data=img)\n",
    "    for k in range(img.shape[0]):\n",
    "        fp.write('{}, {}\\n'.format(os.path.join(data_valid_path, '{:04d}.npz'.format(cnt)), k))\n",
    "    cnt += 1\n",
    "    bar.update(cnt)\n",
    "\n",
    "bar.finish()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencode_meta['min'] = float(autoencode_meta['min'])\n",
    "autoencode_meta['max'] = float(autoencode_meta['max'])\n",
    "with open('ae_meta.json', 'w') as fp:\n",
    "    json.dump(autoencode_meta, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Data Preprocessing\n",
    "\n",
    "***Using the Stanford dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_path = os.path.join(os.path.join('source', '{}'), 'Stanford')\n",
    "se_path = os.path.join(os.path.join('source', '{}'), 'Seattle')\n",
    "\n",
    "st_path = os.path.join(os.path.join(st_path, 'ST_{}_T2_Axial'), 'no_roi')\n",
    "se_path = os.path.join(os.path.join(se_path, 'SE_{}_T2_Axial'), 'no_roi')\n",
    "healthy_path = os.path.join('source', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_files = []\n",
    "class_types = []\n",
    "\n",
    "dipg_path = st_path.format('DIPG', 'DIPG')\n",
    "for _, _, files in os.walk(dipg_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(dipg_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[0][-4:], filename.split('-')[3])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(dipg_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    files = list(filter(lambda x: all([len(y.split('-')) == 5 for y in x]), files))\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([0 for _ in range(len(files))])\n",
    "    \n",
    "dipg_path = se_path.format('DIPG', 'DIPG')\n",
    "for _, _, files in os.walk(dipg_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(se_dipg_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[0][-4:], filename.split('-')[3])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(dipg_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    files = list(filter(lambda x: all([len(y.split('-')) == 5 for y in x]), files))\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([0 for _ in range(len(files))])\n",
    "    \n",
    "ep_path = st_path.format('EP', 'PF-EP')\n",
    "for _, _, files in os.walk(ep_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(ep_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[1][-4:], filename.split('-')[4])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(ep_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([1 for _ in range(len(files))])\n",
    "    \n",
    "ep_path = se_path.format('EP', 'PF-EP')\n",
    "for _, _, files in os.walk(ep_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(se_ep_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[1][-4:], filename.split('-')[4])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(ep_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([1 for _ in range(len(files))])\n",
    "    \n",
    "mb_path = st_path.format('MB', 'PF-MB')\n",
    "for _, _, files in os.walk(mb_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(mb_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[1][-4:], filename.split('-')[4])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(mb_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([2 for _ in range(len(files))])\n",
    "    \n",
    "mb_path = se_path.format('MB', 'PF-MB')\n",
    "for _, _, files in os.walk(mb_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(se_mb_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[1][-4:], filename.split('-')[4])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(mb_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([2 for _ in range(len(files))])\n",
    "    \n",
    "pilo_path = st_path.format('PILO', 'PF-PILO')\n",
    "for _, _, files in os.walk(pilo_path):\n",
    "    files = set(filter(lambda x: not x.startswith('.'), files))\n",
    "    files = sorted(files.intersection(pilo_set))\n",
    "    iids = {}\n",
    "    for filename in files:\n",
    "        key = '{}_{}'.format(filename.split('-')[1][-4:], filename.split('-')[4])\n",
    "        if key not in iids:\n",
    "            iids[key] = []\n",
    "        iids[key].append(os.path.join(pilo_path, filename))\n",
    "    files = [sorted(iids[key]) for key in sorted(iids.keys())]\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([3 for _ in range(len(files))])\n",
    "    \n",
    "for _, _, files in os.walk(healthy_path):\n",
    "    files = set(map(lambda x: os.path.join(healthy_path, x),\n",
    "                       filter(lambda x: not x.startswith('.'), files)))\n",
    "    files = sorted(files.intersection(normal_set))\n",
    "    class_files.extend(files)\n",
    "    class_types.extend([4 for _ in range(len(files))])\n",
    "    \n",
    "train_path, valid_path, train_type, valid_type = train_test_split(class_files, class_types, test_size=0.2,\n",
    "                                                                  random_state=seed, stratify=class_types)\n",
    "valid_path, test_path, valid_type, test_type = train_test_split(valid_path, valid_type, test_size=0.5,\n",
    "                                                                random_state=seed, stratify=valid_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_meta = {'min': float('inf'), 'max': float('-inf')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = os.path.join(classifier_path, 'train')\n",
    "\n",
    "fp = open('clf_train.csv', 'w')\n",
    "fp.write('filepath, plane, class\\n')\n",
    "\n",
    "cnt = 0\n",
    "bar = ProgressBar(maxval=len(train_path), widgets=[Bar('=', '[', ']'), ' ', Percentage()]).start()\n",
    "\n",
    "for path, method in list(zip(train_path, train_type)):\n",
    "    img = None\n",
    "    if method==4:\n",
    "        img = np.load(path)['T2 ax']\n",
    "        dim1 = max(img.shape[:2]) - img.shape[0]\n",
    "        dim2 = max(img.shape[:2]) - img.shape[1]\n",
    "        if dim1 != 0 or dim2 != 0:\n",
    "            pad = np.pad(img, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                               (math.ceil(dim2/2.0),math.floor(dim2/2.0)), (0,0)),\n",
    "                         mode='constant', constant_values=0)\n",
    "            img = resize(pad, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2,0,1))\n",
    "    else:\n",
    "        img = []\n",
    "        for filename in path:\n",
    "            arr = pdc.dcmread(filename).pixel_array\n",
    "            dim1 = max(arr.shape) - arr.shape[0]\n",
    "            dim2 = max(arr.shape) - arr.shape[1]\n",
    "            if dim1 != 0 or dim2 != 0:\n",
    "                arr = np.pad(arr, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                                   (math.ceil(dim2/2.0),math.floor(dim2/2.0))),\n",
    "                             mode='constant', constant_values=0)\n",
    "            img.append(resize(arr, (256, 256), mode='constant',\n",
    "                              clip=True, preserve_range=True).tolist())\n",
    "        img = np.asarray(img)\n",
    "    \n",
    "    img = np.rot90(img, axes=(2,1))\n",
    "    img = (img - img.mean())/img.std()\n",
    "    \n",
    "    min_img = img.min()\n",
    "    max_img = img.max()\n",
    "    if min_img < class_meta['min']:\n",
    "        class_meta['min'] = min_img\n",
    "    if max_img > class_meta['max']:\n",
    "        class_meta['max'] = max_img\n",
    "    \n",
    "    np.savez_compressed(os.path.join(data_train_path, '{:04d}'.format(cnt)), data=img)\n",
    "    for k in range(img.shape[0]):\n",
    "        fp.write('{}, {}, {}\\n'.format(os.path.join(data_train_path, '{:04d}.npz'.format(cnt)), k, method))\n",
    "    cnt += 1\n",
    "    bar.update(cnt)\n",
    "\n",
    "bar.finish()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_meta['min'] = float(class_meta['min'])\n",
    "class_meta['max'] = float(class_meta['max'])\n",
    "with open('clf_meta.json', 'w') as fp:\n",
    "    json.dump(class_meta, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid_path = os.path.join(classifier_path, 'valid')\n",
    "\n",
    "fp = open('clf_valid.csv', 'w')\n",
    "fp.write('filepath, plane, class\\n')\n",
    "\n",
    "cnt = 0\n",
    "bar = ProgressBar(maxval=len(valid_path), widgets=[Bar('=', '[', ']'), ' ', Percentage()]).start()\n",
    "\n",
    "for path, method in list(zip(valid_path, valid_type)):\n",
    "    img = None\n",
    "    if method==4:\n",
    "        img = np.load(path)['T2 ax']\n",
    "        dim1 = max(img.shape[:2]) - img.shape[0]\n",
    "        dim2 = max(img.shape[:2]) - img.shape[1]\n",
    "        if dim1 != 0 or dim2 != 0:\n",
    "            pad = np.pad(img, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                               (math.ceil(dim2/2.0),math.floor(dim2/2.0)), (0,0)),\n",
    "                         mode='constant', constant_values=0)\n",
    "            img = resize(pad, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2,0,1))\n",
    "    else:\n",
    "        img = []\n",
    "        for filename in path:\n",
    "            arr = pdc.dcmread(filename).pixel_array\n",
    "            dim1 = max(arr.shape) - arr.shape[0]\n",
    "            dim2 = max(arr.shape) - arr.shape[1]\n",
    "            if dim1 != 0 or dim2 != 0:\n",
    "                arr = np.pad(arr, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                                   (math.ceil(dim2/2.0),math.floor(dim2/2.0))),\n",
    "                             mode='constant', constant_values=0)\n",
    "            img.append(resize(arr, (256, 256), mode='constant',\n",
    "                              clip=True, preserve_range=True).tolist())\n",
    "        img = np.asarray(img)\n",
    "    \n",
    "    img = np.rot90(img, axes=(2,1))\n",
    "    img = (img - img.mean())/img.std()\n",
    "    \n",
    "    np.savez_compressed(os.path.join(data_valid_path, '{:04d}'.format(cnt)), data=img)\n",
    "    for k in range(img.shape[0]):\n",
    "        fp.write('{}, {}, {}\\n'.format(os.path.join(data_valid_path, '{:04d}.npz'.format(cnt)), k, method))\n",
    "    cnt += 1\n",
    "    bar.update(cnt)\n",
    "\n",
    "bar.finish()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_path = os.path.join(classifier_path, 'test')\n",
    "\n",
    "fp = open('clf_test.csv', 'w')\n",
    "fp.write('filepath, plane, class\\n')\n",
    "\n",
    "cnt = 0\n",
    "bar = ProgressBar(maxval=len(test_path), widgets=[Bar('=', '[', ']'), ' ', Percentage()]).start()\n",
    "\n",
    "for path, method in list(zip(test_path, test_type)):\n",
    "    img = None\n",
    "    if method==4:\n",
    "        img = np.load(path)['T2 ax']\n",
    "        dim1 = max(img.shape[:2]) - img.shape[0]\n",
    "        dim2 = max(img.shape[:2]) - img.shape[1]\n",
    "        if dim1 != 0 or dim2 != 0:\n",
    "            pad = np.pad(img, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                               (math.ceil(dim2/2.0),math.floor(dim2/2.0)), (0,0)),\n",
    "                         mode='constant', constant_values=0)\n",
    "            img = resize(pad, (256, 256), mode='constant', clip=True, preserve_range=True)\n",
    "        img = img.transpose((2,0,1))\n",
    "    else:\n",
    "        img = []\n",
    "        for filename in path:\n",
    "            arr = pdc.dcmread(filename).pixel_array\n",
    "            dim1 = max(arr.shape) - arr.shape[0]\n",
    "            dim2 = max(arr.shape) - arr.shape[1]\n",
    "            if dim1 != 0 or dim2 != 0:\n",
    "                arr = np.pad(arr, ((math.ceil(dim1/2.0),math.floor(dim1/2.0)),\n",
    "                                   (math.ceil(dim2/2.0),math.floor(dim2/2.0))),\n",
    "                             mode='constant', constant_values=0)\n",
    "            img.append(resize(arr, (256, 256), mode='constant',\n",
    "                              clip=True, preserve_range=True).tolist())\n",
    "        img = np.asarray(img)\n",
    "    \n",
    "    img = np.rot90(img, axes=(2,1))\n",
    "    img = (img - img.mean())/img.std()\n",
    "    \n",
    "    np.savez_compressed(os.path.join(data_test_path, '{:04d}'.format(cnt)), data=img)\n",
    "    for k in range(img.shape[0]):\n",
    "        fp.write('{}, {}, {}\\n'.format(os.path.join(data_test_path, '{:04d}.npz'.format(cnt)), k, method))\n",
    "    cnt += 1\n",
    "    bar.update(cnt)\n",
    "\n",
    "bar.finish()\n",
    "fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
